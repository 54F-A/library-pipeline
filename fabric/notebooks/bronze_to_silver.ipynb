{"cells":[{"cell_type":"code","source":["# Cell 1: Install package from GitHub\n","%pip install \"git+https://github.com/54F-A/library-pipeline.git@fabric\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[10,11,12,13,14,15],"state":"finished","livy_statement_state":"available","session_id":"8088fb67-3921-46fa-9fc9-723175d5ed4c","normalized_state":"finished","queued_time":"2025-11-12T15:06:26.1092894Z","session_start_time":null,"execution_start_time":"2025-11-12T15:06:31.8490516Z","execution_finish_time":"2025-11-12T15:06:58.2227401Z","parent_msg_id":"758afc70-75a3-416c-a08c-e911da99f965"},"text/plain":"StatementMeta(, 8088fb67-3921-46fa-9fc9-723175d5ed4c, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/54F-A/library-pipeline.git@fabric\n  Cloning https://github.com/54F-A/library-pipeline.git (to revision fabric) to /tmp/pip-req-build-9tzh5i3m\n  Running command git clone --filter=blob:none --quiet https://github.com/54F-A/library-pipeline.git /tmp/pip-req-build-9tzh5i3m\n  Running command git checkout -b fabric --track origin/fabric\n  Switched to a new branch 'fabric'\n  branch 'fabric' set up to track 'origin/fabric'.\n  Resolved https://github.com/54F-A/library-pipeline.git to commit ac2babd3df2f4a0b75239c0ac86b2cd45704fe89\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25hBuilding wheels for collected packages: library-pipeline\n  Building wheel for library-pipeline (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n\u001b[?25h  Created wheel for library-pipeline: filename=library_pipeline-0.1.0-py3-none-any.whl size=4761 sha256=9a881fb0a35b48785c4b0626098c4d031f766893fe4ca6fbece34f33c95e9734\n  Stored in directory: /tmp/pip-ephem-wheel-cache-8kuw40i4/wheels/0a/79/14/bd28a237b81cd1846a86b629f890a3456d34fc56f593cad407\nSuccessfully built library-pipeline\nInstalling collected packages: library-pipeline\nSuccessfully installed library-pipeline-0.1.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nWarning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b1b818a6-4725-4e9d-b5d4-86a22b5b7496"},{"cell_type":"code","source":["# Cell 2: Import your functions\n","from data_processing.ingestion import load_csv, load_json\n","from data_processing.cleaning import (\n","    remove_duplicates, \n","    handle_missing_values, \n","    standardize_dates\n",")\n","\n","print(\"✅ Package installed and imported successfully!\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"8088fb67-3921-46fa-9fc9-723175d5ed4c","normalized_state":"finished","queued_time":"2025-11-12T15:08:28.948786Z","session_start_time":null,"execution_start_time":"2025-11-12T15:08:32.7895745Z","execution_finish_time":"2025-11-12T15:08:33.1566227Z","parent_msg_id":"ab939c5f-7f62-47da-a3a5-c7a0e0870bc8"},"text/plain":"StatementMeta(, 8088fb67-3921-46fa-9fc9-723175d5ed4c, 17, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Package installed and imported successfully!\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2216ac2a-c5ad-484e-b12a-ed19032f9cfd"},{"cell_type":"code","source":["# Cell 3: Load data from Lakehouse Files\n","import pandas as pd\n","\n","# Read CSV from Files\n","file_path = \"/lakehouse/default/Files/bronze/circulation_data.csv\"\n","df_raw = pd.read_csv(file_path)\n","\n","print(f\"Loaded {len(df_raw)} rows\")\n","print(df_raw.head())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"8088fb67-3921-46fa-9fc9-723175d5ed4c","normalized_state":"finished","queued_time":"2025-11-12T15:10:21.8058535Z","session_start_time":null,"execution_start_time":"2025-11-12T15:10:21.8070538Z","execution_finish_time":"2025-11-12T15:10:24.2725106Z","parent_msg_id":"c1ec387d-616e-4078-b409-5bdbca573a51"},"text/plain":"StatementMeta(, 8088fb67-3921-46fa-9fc9-723175d5ed4c, 18, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded 5100 rows\n  transaction_id member_id               isbn checkout_date return_date  \\\n0      TXN000000    M93810  978-0-433-21819-7    2024-08-17  2024-08-25   \n1      TXN000001    M28289  978-0-338-90838-4    2024-08-15  2024-09-02   \n2      TXN000002    M21395  978-1-02-654235-4    2023-12-27         NaN   \n3      TXN000003    M38657  978-0-559-40781-9    2025-09-24         NaN   \n4      TXN000004    M36062  978-0-8495-9310-9    2025-02-08  2025-02-16   \n\n  branch_id  \n0     BR012  \n1     BR011  \n2     BR001  \n3     BR010  \n4     BR012  \n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bb9575ad-e55d-40d4-93c8-dfd19bdec334"},{"cell_type":"code","source":["# Cell 4: Apply your cleaning functions (BRONZE → SILVER)\n","print(\"Applying data cleaning pipeline...\")\n","\n","# Remove duplicates\n","df_clean = remove_duplicates(df_raw, subset=['transaction_id'])\n","print(f\"After removing duplicates: {len(df_clean)} rows\")\n","\n","# Handle missing values\n","df_clean = handle_missing_values(df_clean, strategy='drop')\n","print(f\"After handling missing values: {len(df_clean)} rows\")\n","\n","# Standardize dates\n","df_clean = standardize_dates(df_clean, ['checkout_date', 'return_date'])\n","print(\"Dates standardized\")\n","\n","print(f\"\\n✅ Cleaning complete! {len(df_raw)} → {len(df_clean)} rows\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"8088fb67-3921-46fa-9fc9-723175d5ed4c","normalized_state":"finished","queued_time":"2025-11-12T15:10:50.0278642Z","session_start_time":null,"execution_start_time":"2025-11-12T15:10:50.0288504Z","execution_finish_time":"2025-11-12T15:10:50.4603631Z","parent_msg_id":"ec1db1e4-34b7-41aa-b613-0dfef52fcbe1"},"text/plain":"StatementMeta(, 8088fb67-3921-46fa-9fc9-723175d5ed4c, 19, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Applying data cleaning pipeline...\nAfter removing duplicates: 5000 rows\nAfter handling missing values: 4227 rows\nDates standardized\n\n✅ Cleaning complete! 5100 → 4227 rows\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"05e445c2-3ca6-4d4a-afa3-75195bde8f2c"},{"cell_type":"code","source":["# Cell 5: Save as Delta table (SILVER layer)\n","# Convert pandas to Spark DataFrame\n","df_spark = spark.createDataFrame(df_clean)\n","\n","# Write as Delta table\n","table_name = \"silver_circulation\"\n","df_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n","\n","print(f\"✅ Created Delta table: {table_name}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"8088fb67-3921-46fa-9fc9-723175d5ed4c","normalized_state":"finished","queued_time":"2025-11-12T15:12:34.3104925Z","session_start_time":null,"execution_start_time":"2025-11-12T15:12:34.3116379Z","execution_finish_time":"2025-11-12T15:12:59.2448375Z","parent_msg_id":"80a60b2d-1f46-4727-b3a7-11be911933b2"},"text/plain":"StatementMeta(, 8088fb67-3921-46fa-9fc9-723175d5ed4c, 20, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Created Delta table: silver_circulation\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0656d221-b6ad-444e-9328-b264fd1fef36"},{"cell_type":"code","source":["# Cell 6: Query the Delta table\n","query = f\"\"\"\n","SELECT \n","    COUNT(*) as total_transactions,\n","    COUNT(DISTINCT member_id) as unique_members,\n","    COUNT(DISTINCT isbn) as unique_books,\n","    COUNT(DISTINCT branch_id) as branches\n","FROM {table_name}\n","\"\"\"\n","\n","result = spark.sql(query)\n","result.show()\n","\n","print(\"✅ Silver layer ready for analysis!\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":21,"statement_ids":[21],"state":"finished","livy_statement_state":"available","session_id":"8088fb67-3921-46fa-9fc9-723175d5ed4c","normalized_state":"finished","queued_time":"2025-11-12T15:14:30.4851259Z","session_start_time":null,"execution_start_time":"2025-11-12T15:14:30.4862973Z","execution_finish_time":"2025-11-12T15:14:38.3717038Z","parent_msg_id":"36f9e888-a492-4208-a68a-f48a3b956496"},"text/plain":"StatementMeta(, 8088fb67-3921-46fa-9fc9-723175d5ed4c, 21, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+------------------+--------------+------------+--------+\n|total_transactions|unique_members|unique_books|branches|\n+------------------+--------------+------------+--------+\n|              4227|          4127|        4227|      30|\n+------------------+--------------+------------+--------+\n\n✅ Silver layer ready for analysis!\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d0ce1a5c-b597-4368-a701-5eeaccaa2990"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"50bdf863-d197-41de-a977-75193a306e2d","known_lakehouses":[{"id":"50bdf863-d197-41de-a977-75193a306e2d"}],"default_lakehouse_name":"library_pipeline","default_lakehouse_workspace_id":"aee508b9-e2b0-4921-86ab-34e94863be6c"}}},"nbformat":4,"nbformat_minor":5}